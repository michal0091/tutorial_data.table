# Optimizaci√≥n de Performance

::: {.callout-tip icon="false"}
## En este cap√≠tulo dominar√°s
- **Configuraci√≥n de threading** para aprovechar m√∫ltiples n√∫cleos
- **Keys e √≠ndices** para b√∫squedas ultra-r√°pidas
- **Profiling y benchmarking** sistem√°tico de c√≥digo
- **Optimizaci√≥n de memoria** y gesti√≥n eficiente de recursos
- **Identificaci√≥n de cuellos de botella** en pipelines complejos
- **T√©cnicas espec√≠ficas** para datasets grandes (>1M filas)
:::

```{r}
#| label: setup-cap04-performance
#| include: false

library(data.table)
library(microbenchmark)
library(ggplot2)
library(knitr)
library(parallel)

# Configuraci√≥n
options(datatable.print.nrows = 8)
options(datatable.print.class = TRUE)

# Datasets para performance testing
set.seed(2024)

# Dataset grande para benchmarking
big_dataset <- data.table(
  id = sample(1:1000000, 2000000, replace = TRUE),
  group_major = sample(LETTERS[1:50], 2000000, replace = TRUE),
  group_minor = sample(letters[1:10], 2000000, replace = TRUE),
  value_numeric = rnorm(2000000, 100, 25),
  value_integer = sample(1:1000, 2000000, replace = TRUE),
  timestamp = sample(seq.POSIXt(as.POSIXct("2020-01-01"), as.POSIXct("2024-12-31"), by = "min"), 2000000, replace = TRUE),
  category = sample(paste0("Cat_", 1:20), 2000000, replace = TRUE),
  status = sample(c("Active", "Inactive", "Pending", "Completed"), 2000000, replace = TRUE, prob = c(0.4, 0.3, 0.2, 0.1)),
  region = sample(c("North", "South", "East", "West", "Central"), 2000000, replace = TRUE),
  amount = round(exp(rnorm(2000000, 5, 1.5)), 2)
)

# Dataset de lookup para joins
lookup_data <- data.table(
  id = 1:1000000,
  entity_name = paste0("Entity_", sprintf("%07d", 1:1000000)),
  entity_type = sample(c("Premium", "Standard", "Basic", "VIP"), 1000000, replace = TRUE),
  weight_factor = round(runif(1000000, 0.5, 2.0), 3),
  creation_date = sample(seq(as.Date("2019-01-01"), as.Date("2024-01-01"), by = "day"), 1000000, replace = TRUE),
  is_active = sample(c(TRUE, FALSE), 1000000, replace = TRUE, prob = c(0.8, 0.2))
)

# Dataset temporal para an√°lisis de series de tiempo
temporal_dataset <- data.table(
  timestamp = seq(as.POSIXct("2023-01-01 00:00:00"), 
                 as.POSIXct("2024-12-31 23:59:59"), 
                 by = "5 min"),
  device_id = rep(paste0("DEV_", sprintf("%04d", 1:500)), length.out = 210240),
  sensor_temp = round(20 + 10*sin(seq(0, 20*pi, length.out = 210240)) + rnorm(210240, 0, 2), 1),
  sensor_pressure = round(1013 + 50*cos(seq(0, 15*pi, length.out = 210240)) + rnorm(210240, 0, 10), 1),
  sensor_humidity = round(50 + 30*sin(seq(0, 25*pi, length.out = 210240)) + rnorm(210240, 0, 5), 1),
  quality_flag = sample(c("OK", "WARNING", "ERROR"), 210240, replace = TRUE, prob = c(0.85, 0.12, 0.03))
)

cat("Datasets creados para performance testing:\n")
cat("‚Ä¢ big_dataset:", nrow(big_dataset), "filas,", ncol(big_dataset), "columnas\n")
cat("‚Ä¢ lookup_data:", nrow(lookup_data), "filas,", ncol(lookup_data), "columnas\n")
cat("‚Ä¢ temporal_dataset:", nrow(temporal_dataset), "filas,", ncol(temporal_dataset), "columnas\n")
```

## Configuraci√≥n de Threading para M√∫ltiples N√∫cleos

El threading autom√°tico de `data.table` puede acelerar dram√°ticamente las operaciones en m√°quinas multi-core.

### 1. **Configuraci√≥n √ìptima de Threads**

```{r}
#| label: threading-configuracion
#| echo: true

# Evaluar configuraci√≥n del sistema
cat("=== CONFIGURACI√ìN DEL SISTEMA ===\n")
cat("CPU cores disponibles:", parallel::detectCores(), "\n")
cat("CPU cores con hyperthreading:", parallel::detectCores(logical = TRUE), "\n")
cat("Threads configurados en data.table:", getDTthreads(), "\n")

# Funci√≥n para determinar configuraci√≥n √≥ptima
determine_optimal_threads <- function() {
  max_cores <- parallel::detectCores(logical = FALSE)  # Cores f√≠sicos
  
  if(max_cores <= 2) {
    return(max_cores)
  } else if(max_cores <= 4) {
    return(max_cores)
  } else if(max_cores <= 8) {
    return(max_cores - 1)  # Dejar un core libre
  } else {
    return(min(8, max_cores - 2))  # Para sistemas muy grandes, no usar todos
  }
}

optimal_threads <- determine_optimal_threads()
cat("Configuraci√≥n recomendada:", optimal_threads, "threads\n")

# Aplicar configuraci√≥n √≥ptima
setDTthreads(optimal_threads)
cat("Configuraci√≥n aplicada:", getDTthreads(), "threads\n")
```

### 2. **Benchmark de Threading Performance**

```{r}
#| label: threading-benchmark
#| echo: true
#| cache: true

# Funci√≥n para benchmark con diferentes configuraciones de threads
benchmark_threading <- function(n_threads, dataset_size = 500000) {
  setDTthreads(n_threads)
  dt_sample <- big_dataset[sample(.N, dataset_size)]
  
  # Operaciones que se benefician del threading
  tiempo_agregacion <- system.time({
    result_agg <- dt_sample[, .(
      mean_value = mean(value_numeric),
      sum_amount = sum(amount),
      count_records = .N,
      median_value = median(value_numeric)
    ), by = .(group_major, group_minor)]
  })
  
  tiempo_sort <- system.time({
    result_sort <- dt_sample[order(-value_numeric, group_major)]
  })
  
  return(list(
    threads = n_threads,
    agregacion = tiempo_agregacion[3],
    ordenamiento = tiempo_sort[3],
    total = tiempo_agregacion[3] + tiempo_sort[3]
  ))
}

# Comparar diferentes configuraciones
configuraciones_threads <- c(1, 2, 4, min(8, parallel::detectCores()))
resultados_threads <- list()

cat("=== BENCHMARK DE THREADING ===\n")
for(i in seq_along(configuraciones_threads)) {
  n_threads <- configuraciones_threads[i]
  cat("Probando con", n_threads, "thread(s)... ")
  
  resultado <- benchmark_threading(n_threads, 300000)  # Dataset m√°s peque√±o para rapidez
  resultados_threads[[i]] <- resultado
  
  cat("Agregaci√≥n:", round(resultado$agregacion, 3), "s, ",
      "Ordenamiento:", round(resultado$ordenamiento, 3), "s, ",
      "Total:", round(resultado$total, 3), "s\n")
}

# Crear tabla de resultados
tabla_threads <- rbindlist(resultados_threads)
print("\nComparaci√≥n de performance por n√∫mero de threads:")
print(tabla_threads)

# Calcular speedup relativo al baseline (1 thread)
baseline <- tabla_threads[threads == 1, total]
tabla_threads[, speedup := round(baseline / total, 2)]
print("\nSpeedup relativo (vs 1 thread):")
print(tabla_threads[, .(threads, total, speedup)])

# Restaurar configuraci√≥n √≥ptima
setDTthreads(optimal_threads)
```

## Keys e √çndices: La Base de la Velocidad

### 1. **Setkey: Ordenamiento F√≠sico para Velocidad**

```{r}
#| label: setkey-performance
#| echo: true

# Comparar performance con y sin keys
dt_no_key <- copy(big_dataset[sample(.N, 500000)])
dt_with_key <- copy(dt_no_key)

cat("=== COMPARACI√ìN SETKEY ===\n")

# Tiempo para establecer key
tiempo_setkey <- system.time(setkey(dt_with_key, group_major, group_minor))
cat("Tiempo para establecer key:", round(tiempo_setkey[3], 3), "segundos\n")

# Comparar b√∫squedas simples
valores_busqueda <- c("A", "B", "C", "D", "E")
sub_valores <- c("a", "b", "c")

tiempo_sin_key <- system.time({
  result_no_key <- dt_no_key[group_major %in% valores_busqueda & group_minor %in% sub_valores]
})

tiempo_con_key <- system.time({
  result_with_key <- dt_with_key[.(valores_busqueda, sub_valores)]
})

cat("B√∫squeda sin key:", round(tiempo_sin_key[3], 4), "segundos\n")
cat("B√∫squeda con key:", round(tiempo_con_key[3], 4), "segundos\n")
cat("Speedup:", round(tiempo_sin_key[3] / tiempo_con_key[3], 1), "x m√°s r√°pido\n")

# Verificar que ambos resultados son equivalentes
cat("Resultados equivalentes:", nrow(result_no_key) == nrow(result_with_key), "\n")
```

### 2. **M√∫ltiples Keys para Diferentes Patrones de Consulta**

```{r}
#| label: multiple-keys-strategy
#| echo: true

# Crear m√∫ltiples copias para diferentes estrategias de indexing
dt_by_group <- copy(big_dataset[sample(.N, 300000)])
dt_by_time <- copy(dt_by_group)
dt_by_id <- copy(dt_by_group)

# Establecer diferentes keys seg√∫n el patr√≥n de uso
setkey(dt_by_group, group_major, group_minor)
setkey(dt_by_time, timestamp)
setkey(dt_by_id, id)

cat("=== ESTRATEGIAS DE KEYS ===\n")
cat("dt_by_group key:", paste(key(dt_by_group), collapse = ", "), "\n")
cat("dt_by_time key:", paste(key(dt_by_time), collapse = ", "), "\n")
cat("dt_by_id key:", paste(key(dt_by_id), collapse = ", "), "\n\n")

# Consultas optimizadas seg√∫n la key
cat("Consultando por grupos...\n")
tiempo_grupo <- system.time({
  result_grupo <- dt_by_group[.("A", c("a", "b", "c"))]
})

cat("Consultando por tiempo...\n") 
tiempo_temporal <- system.time({
  result_temporal <- dt_by_time[timestamp >= as.POSIXct("2024-01-01") & 
                               timestamp < as.POSIXct("2024-02-01")]
})

cat("Consultando por IDs...\n")
ids_especificos <- sample(1:1000000, 1000)
tiempo_ids <- system.time({
  result_ids <- dt_by_id[.(ids_especificos)]
})

cat("Tiempos de consulta optimizada:\n")
cat("‚Ä¢ Por grupos:", round(tiempo_grupo[3], 4), "segundos\n")
cat("‚Ä¢ Por tiempo:", round(tiempo_temporal[3], 4), "segundos\n") 
cat("‚Ä¢ Por IDs:", round(tiempo_ids[3], 4), "segundos\n")
```

### 3. **√çndices Secundarios con setindex()**

```{r}
#| label: setindex-secondary
#| echo: true

# Crear tabla con key principal e √≠ndices secundarios
dt_indexed <- copy(big_dataset[sample(.N, 400000)])
setkey(dt_indexed, group_major)  # Key principal

cat("=== √çNDICES SECUNDARIOS ===\n")

# Crear √≠ndices secundarios para consultas frecuentes
cat("Creando √≠ndices secundarios...\n")
tiempo_indices <- system.time({
  setindex(dt_indexed, category)
  setindex(dt_indexed, status)
  setindex(dt_indexed, region)
  setindex(dt_indexed, timestamp)
  setindex(dt_indexed, id, value_numeric)  # √çndice compuesto
})

cat("Tiempo para crear √≠ndices:", round(tiempo_indices[3], 3), "segundos\n")
cat("√çndices creados:", length(indices(dt_indexed)), "\n")
print(indices(dt_indexed))

# Comparar consultas con y sin √≠ndices
dt_sin_indices <- copy(big_dataset[sample(.N, 400000)])

# Consulta que puede usar √≠ndice
cat("\nComparando consultas por categor√≠a:\n")
tiempo_sin_indice <- system.time({
  result_sin_indice <- dt_sin_indices[category == "Cat_5" & status == "Active"]
})

tiempo_con_indice <- system.time({
  result_con_indice <- dt_indexed[category == "Cat_5" & status == "Active"]
})

cat("Sin √≠ndice:", round(tiempo_sin_indice[3], 4), "segundos\n")
cat("Con √≠ndice:", round(tiempo_con_indice[3], 4), "segundos\n")
cat("Speedup:", round(tiempo_sin_indice[3] / tiempo_con_indice[3], 1), "x\n")
```

## Profiling y Benchmarking Sistem√°tico

### 1. **Modo Verbose para An√°lisis Detallado**

```{r}
#| label: verbose-analysis
#| echo: true

# Activar modo verbose para operaciones espec√≠ficas
verbose_analysis <- function(dt, operation_name, operation_func) {
  cat("=== AN√ÅLISIS:", operation_name, "===\n")
  
  # Activar verbose temporalmente
  old_verbose <- getOption("datatable.verbose")
  options(datatable.verbose = TRUE)
  
  # Ejecutar operaci√≥n
  start_time <- Sys.time()
  result <- operation_func(dt)
  end_time <- Sys.time()
  
  # Restaurar verbose
  options(datatable.verbose = old_verbose)
  
  cat("Tiempo total:", round(as.numeric(end_time - start_time), 4), "segundos\n")
  cat("Filas resultado:", nrow(result), "\n\n")
  
  return(result)
}

# Ejemplo de an√°lisis con verbose
dt_sample <- big_dataset[sample(.N, 100000)]

# Operaci√≥n compleja para analizar
resultado_verbose <- verbose_analysis(dt_sample, "Agregaci√≥n Compleja", function(dt) {
  dt[status %in% c("Active", "Completed"), 
     .(avg_value = mean(value_numeric),
       sum_amount = sum(amount),
       count = .N,
       median_amount = median(amount)), 
     by = .(group_major, category)]
})

print(head(resultado_verbose))
```

### 2. **Benchmarking Comparativo de Estrategias**

```{r}
#| label: benchmark-estrategias
#| echo: true
#| cache: true

# Crear funci√≥n de benchmark comprehensiva
benchmark_comprehensive <- function(dt_size = 200000) {
  dt_test <- big_dataset[sample(.N, dt_size)]
  
  # Estrategia 1: Sin optimizaciones
  strategy1 <- function() {
    dt_test[group_major %in% c("A", "B", "C") & status == "Active",
           .(mean_val = mean(value_numeric), 
             sum_amount = sum(amount),
             count = .N),
           by = .(group_minor, category)]
  }
  
  # Estrategia 2: Con setkey optimizado
  dt_keyed <- copy(dt_test)
  setkey(dt_keyed, group_major, group_minor)
  strategy2 <- function() {
    dt_keyed[.(c("A", "B", "C"))][status == "Active",
            .(mean_val = mean(value_numeric),
              sum_amount = sum(amount), 
              count = .N),
            by = .(group_minor, category)]
  }
  
  # Estrategia 3: Pre-filtrar luego agrupar
  strategy3 <- function() {
    dt_filtered <- dt_test[group_major %in% c("A", "B", "C") & status == "Active"]
    dt_filtered[, .(mean_val = mean(value_numeric),
                   sum_amount = sum(amount),
                   count = .N),
               by = .(group_minor, category)]
  }
  
  # Estrategia 4: Con √≠ndices secundarios
  dt_indexed <- copy(dt_test)
  setindex(dt_indexed, group_major)
  setindex(dt_indexed, status)
  strategy4 <- function() {
    dt_indexed[group_major %in% c("A", "B", "C") & status == "Active",
              .(mean_val = mean(value_numeric),
                sum_amount = sum(amount),
                count = .N),
              by = .(group_minor, category)]
  }
  
  # Ejecutar benchmark
  benchmark_result <- microbenchmark(
    "Sin optimizar" = strategy1(),
    "Con setkey" = strategy2(),
    "Pre-filtrar" = strategy3(), 
    "Con √≠ndices" = strategy4(),
    times = 10
  )
  
  return(benchmark_result)
}

# Ejecutar benchmark comprehensivo
cat("=== BENCHMARK COMPREHENSIVO DE ESTRATEGIAS ===\n")
benchmark_result <- benchmark_comprehensive(150000)
print(benchmark_result)

# Crear visualizaci√≥n si ggplot2 est√° disponible
if(require(ggplot2, quietly = TRUE)) {
  plot_benchmark <- autoplot(benchmark_result) +
    labs(title = "Comparaci√≥n de Estrategias de Optimizaci√≥n",
         subtitle = "Menor tiempo = mejor performance",
         y = "Tiempo (milisegundos)",
         x = "Estrategia") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(plot_benchmark)
}

# An√°lisis de resultados
summary_benchmark <- summary(benchmark_result)
print("\nResumen de performance:")
print(summary_benchmark)

# Calcular speedup relativo
baseline_median <- summary_benchmark[summary_benchmark$expr == "Sin optimizar", "median"]
summary_benchmark$speedup <- round(baseline_median / summary_benchmark$median, 2)
print("\nSpeedup relativo (vs sin optimizar):")
print(summary_benchmark[, c("expr", "median", "speedup")])
```

### 3. **Memory Profiling Avanzado**

```{r}
#| label: memory-profiling-advanced
#| echo: true

# Funci√≥n para an√°lisis detallado de memoria
memory_analysis <- function(operation_name, operation_func, dt_input) {
  cat("=== AN√ÅLISIS DE MEMORIA:", operation_name, "===\n")
  
  # Limpiar garbage collector
  invisible(gc(verbose = FALSE))
  
  # Memoria antes
  mem_before <- as.numeric(object.size(dt_input))
  
  # Ejecutar operaci√≥n y medir tiempo
  start_time <- Sys.time()
  result <- operation_func(dt_input)
  end_time <- Sys.time()
  
  # Memoria despu√©s
  mem_after <- as.numeric(object.size(dt_input))
  mem_result <- as.numeric(object.size(result))
  
  # Reportar resultados
  cat("Tiempo de ejecuci√≥n:", round(as.numeric(end_time - start_time), 4), "segundos\n")
  cat("Memoria input:", format(mem_before, units = "auto"), "\n")
  cat("Memoria despu√©s:", format(mem_after, units = "auto"), "\n")
  cat("Memoria resultado:", format(mem_result, units = "auto"), "\n")
  cat("Cambio en memoria input:", format(mem_after - mem_before, units = "auto"), "\n")
  cat("Eficiencia memoria:", round(mem_result / mem_before * 100, 1), "% del input\n\n")
  
  return(result)
}

# Comparar diferentes operaciones
dt_mem_test <- big_dataset[sample(.N, 100000)]

# Operaci√≥n 1: Modificaci√≥n por referencia
result1 <- memory_analysis("Modificaci√≥n por referencia", function(dt) {
  dt[, new_computed_col := value_numeric * amount * 1.1]
  return(dt)
}, copy(dt_mem_test))

# Operaci√≥n 2: Crear nueva tabla
result2 <- memory_analysis("Crear nueva tabla", function(dt) {
  dt[, .(id, group_major, value_numeric, amount, 
         new_computed_col = value_numeric * amount * 1.1)]
}, dt_mem_test)

# Operaci√≥n 3: Agregaci√≥n
result3 <- memory_analysis("Agregaci√≥n por grupos", function(dt) {
  dt[, .(mean_value = mean(value_numeric),
         sum_amount = sum(amount),
         count = .N), 
     by = .(group_major, group_minor)]
}, dt_mem_test)
```

## Optimizaci√≥n de Operaciones Espec√≠ficas

### 1. **Joins a Gran Escala**

```{r}
#| label: optimize-large-joins
#| echo: true

# Preparar datos para joins de diferentes tama√±os
dt_left_large <- big_dataset[sample(.N, 200000)]
dt_right_large <- lookup_data[sample(.N, 100000)]

cat("=== OPTIMIZACI√ìN DE JOINS GRANDES ===\n")
cat("Tabla izquierda:", nrow(dt_left_large), "filas\n")
cat("Tabla derecha:", nrow(dt_right_large), "filas\n\n")

# Estrategia 1: Merge b√°sico
tiempo_merge <- system.time({
  result_merge <- merge(dt_left_large, dt_right_large, by = "id", all.x = TRUE)
})

# Estrategia 2: Join con setkey
dt_left_key <- copy(dt_left_large)
dt_right_key <- copy(dt_right_large)
setkey(dt_left_key, id)
setkey(dt_right_key, id)

tiempo_setkey_join <- system.time({
  result_setkey <- dt_right_key[dt_left_key]
})

# Estrategia 3: Join con on= (sin modificar tablas originales)
tiempo_on_join <- system.time({
  result_on <- dt_left_large[dt_right_large, on = .(id)]
})

# Estrategia 4: Join filtrado (cuando sabemos que solo necesitamos subset)
ids_relevantes <- intersect(dt_left_large$id, dt_right_large$id)[1:50000]
tiempo_filtered_join <- system.time({
  dt_left_filtered <- dt_left_large[id %in% ids_relevantes]
  dt_right_filtered <- dt_right_large[id %in% ids_relevantes]
  result_filtered <- merge(dt_left_filtered, dt_right_filtered, by = "id")
})

# Comparar resultados
cat("Resultados de joins:\n")
cat("‚Ä¢ Merge b√°sico:", round(tiempo_merge[3], 4), "segundos,", nrow(result_merge), "filas\n")
cat("‚Ä¢ Con setkey:", round(tiempo_setkey_join[3], 4), "segundos,", nrow(result_setkey), "filas\n")
cat("‚Ä¢ Con on=:", round(tiempo_on_join[3], 4), "segundos,", nrow(result_on), "filas\n")
cat("‚Ä¢ Join filtrado:", round(tiempo_filtered_join[3], 4), "segundos,", nrow(result_filtered), "filas\n")

# Mejor estrategia
tiempos_join <- c(tiempo_merge[3], tiempo_setkey_join[3], tiempo_on_join[3], tiempo_filtered_join[3])
mejor_join <- which.min(tiempos_join)
estrategias_join <- c("Merge b√°sico", "Con setkey", "Con on=", "Join filtrado")
cat("\nMejor estrategia:", estrategias_join[mejor_join], "\n")
```

### 2. **Operaciones Temporales Optimizadas**

```{r}
#| label: optimize-temporal-operations
#| echo: true

# Optimizar consultas en datos temporales
dt_temporal <- copy(temporal_dataset[sample(.N, 50000)])

cat("=== OPTIMIZACI√ìN DE CONSULTAS TEMPORALES ===\n")

# Consulta 1: Rango de fechas sin optimizar
tiempo_temporal_sin_key <- system.time({
  result_no_key <- dt_temporal[timestamp >= as.POSIXct("2024-06-01") & 
                              timestamp < as.POSIXct("2024-07-01")]
})

# Consulta 2: Con key temporal
dt_temporal_keyed <- copy(dt_temporal)
setkey(dt_temporal_keyed, timestamp)

tiempo_temporal_con_key <- system.time({
  inicio <- as.POSIXct("2024-06-01")
  fin <- as.POSIXct("2024-07-01")
  result_with_key <- dt_temporal_keyed[timestamp %between% c(inicio, fin)]
})

# Consulta 3: Con rolling joins (para datos temporales complejos)
# Simular eventos de referencia
eventos_ref <- data.table(
  event_time = seq(as.POSIXct("2024-06-01"), as.POSIXct("2024-06-30"), by = "day"),
  event_type = sample(c("A", "B", "C"), 30, replace = TRUE)
)
setkey(eventos_ref, event_time)

tiempo_rolling_join <- system.time({
  result_rolling <- dt_temporal_keyed[eventos_ref, roll = TRUE]
})

cat("Resultados de consultas temporales:\n")
cat("‚Ä¢ Sin key:", round(tiempo_temporal_sin_key[3], 4), "segundos,", nrow(result_no_key), "filas\n")
cat("‚Ä¢ Con key:", round(tiempo_temporal_con_key[3], 4), "segundos,", nrow(result_with_key), "filas\n")
cat("‚Ä¢ Rolling join:", round(tiempo_rolling_join[3], 4), "segundos,", nrow(result_rolling), "filas\n")
```

## Casos de Uso de Optimizaci√≥n Extrema

### 1. **Pipeline de An√°lisis de Alto Rendimiento**

```{r}
#| label: high-performance-pipeline
#| echo: true

# Pipeline optimizado para an√°lisis complejo
create_optimized_pipeline <- function(dt, sample_size = 100000) {
  cat("=== PIPELINE DE ALTO RENDIMIENTO ===\n")
  
  # Paso 1: Muestreo estratificado eficiente
  dt_sample <- dt[, .SD[sample(min(.N, sample_size), sample_size)], by = region]
  
  # Paso 2: Establecer key √≥ptima para operaciones posteriores
  setkey(dt_sample, group_major, group_minor)
  
  # Paso 3: C√°lculos intermedios optimizados (por referencia)
  dt_sample[, `:=`(
    value_normalized = scale(value_numeric)[,1],
    amount_log = log1p(amount),  # log1p es m√°s estable que log
    efficiency_ratio = value_numeric / (amount + 1),
    timestamp_hour = hour(timestamp)
  )]
  
  # Paso 4: Agregaciones complejas usando .SD optimizado
  result_aggregated <- dt_sample[, 
    .(
      # Estad√≠sticas b√°sicas
      count = .N,
      mean_value = mean(value_normalized, na.rm = TRUE),
      median_amount = median(amount_log, na.rm = TRUE),
      
      # Estad√≠sticas avanzadas
      p95_efficiency = quantile(efficiency_ratio, 0.95, na.rm = TRUE),
      cv_value = sd(value_normalized, na.rm = TRUE) / abs(mean(value_normalized, na.rm = TRUE)),
      
      # An√°lisis temporal
      peak_hour = timestamp_hour[which.max(value_numeric)],
      active_hours = uniqueN(timestamp_hour),
      
      # Diversidad
      categories_used = uniqueN(category),
      status_diversity = uniqueN(status)
    ),
    by = .(region, group_major),
    .SDcols = c("value_normalized", "amount_log", "efficiency_ratio", 
                "timestamp_hour", "value_numeric", "category", "status")
  ]
  
  # Paso 5: Post-procesamiento optimizado
  result_aggregated[, `:=`(
    performance_score = round((mean_value + p95_efficiency) * log1p(count), 2),
    complexity_index = categories_used * status_diversity * active_hours
  )]
  
  # Paso 6: Ranking y clasificaci√≥n final
  result_aggregated[, rank_performance := frank(-performance_score), by = region]
  result_aggregated[, tier := fcase(
    rank_performance <= 3, "Tier_1",
    rank_performance <= 10, "Tier_2", 
    rank_performance <= 20, "Tier_3",
    default = "Tier_4"
  )]
  
  return(result_aggregated[order(-performance_score)])
}

# Ejecutar pipeline optimizado
tiempo_pipeline <- system.time({
  resultado_pipeline <- create_optimized_pipeline(big_dataset, 80000)
})

cat("Tiempo total del pipeline:", round(tiempo_pipeline[3], 3), "segundos\n")
cat("Registros procesados: ~80,000 ‚Üí ", nrow(resultado_pipeline), "grupos finales\n")
cat("Reducci√≥n de datos:", round((1 - nrow(resultado_pipeline)/80000) * 100, 1), "%\n\n")

print("Top 10 grupos por performance:")
print(resultado_pipeline[1:10, .(region, group_major, count, performance_score, tier)])
```

### 2. **Sistema de Monitoreo de Performance en Tiempo Real**

```{r}
#| label: real-time-performance-monitoring
#| echo: true

# Sistema para monitorear performance de operaciones data.table
performance_monitor <- function() {
  # Crear registro de operaciones
  operations_log <- data.table(
    operation_id = character(),
    operation_type = character(),
    dataset_size = integer(),
    execution_time = numeric(),
    memory_used = numeric(),
    threads_used = integer(),
    timestamp = .POSIXct(numeric())
  )
  
  # Funci√≥n para registrar operaci√≥n
  log_operation <- function(op_type, dt_size, exec_time, mem_usage) {
    new_entry <- data.table(
      operation_id = paste0(op_type, "_", format(Sys.time(), "%H%M%S")),
      operation_type = op_type,
      dataset_size = dt_size,
      execution_time = exec_time,
      memory_used = mem_usage,
      threads_used = getDTthreads(),
      timestamp = Sys.time()
    )
    operations_log <<- rbindlist(list(operations_log, new_entry), use.names = TRUE, fill = TRUE, ignore.attr = TRUE)
  }
  
  # Funci√≥n para analizar performance
  analyze_performance <- function() {
    if(nrow(operations_log) == 0) {
      cat("No hay operaciones registradas\n")
      return(NULL)
    }
    
    # An√°lisis por tipo de operaci√≥n
    performance_summary <- operations_log[, .(
      operations_count = .N,
      avg_time = round(mean(execution_time), 4),
      median_time = round(median(execution_time), 4),
      max_time = round(max(execution_time), 4),
      avg_memory = round(mean(memory_used), 0),
      throughput_rows_per_sec = round(mean(dataset_size / execution_time), 0)
    ), by = operation_type]
    
    return(performance_summary)
  }
  
  return(list(log = log_operation, analyze = analyze_performance, get_log = function() operations_log))
}

# Inicializar sistema de monitoreo
monitor <- performance_monitor()

# Simular diferentes operaciones y monitorearlas
dt_test <- big_dataset[sample(.N, 50000)]

# Operaci√≥n 1: Agregaci√≥n
cat("Monitoreando operaciones:\n")
tiempo_agg <- system.time({
  result_agg <- dt_test[, .(mean_val = mean(value_numeric)), by = group_major]
})
monitor$log("aggregation", nrow(dt_test), tiempo_agg[3], object.size(result_agg))

# Operaci√≥n 2: Join
tiempo_join <- system.time({
  result_join <- dt_test[lookup_data[1:10000], on = .(id)]
})
monitor$log("join", nrow(dt_test), tiempo_join[3], object.size(result_join))

# Operaci√≥n 3: Sort
tiempo_sort <- system.time({
  result_sort <- dt_test[order(-value_numeric)]
})
monitor$log("sort", nrow(dt_test), tiempo_sort[3], object.size(result_sort))

# An√°lisis de performance
cat("\n=== AN√ÅLISIS DE PERFORMANCE ===\n")
performance_analysis <- monitor$analyze()
print(performance_analysis)

# Identificar operaciones problem√°ticas
if(!is.null(performance_analysis)) {
  problematic_ops <- performance_analysis[avg_time > median(avg_time) * 2]
  if(nrow(problematic_ops) > 0) {
    cat("\n‚ö†Ô∏è Operaciones con performance sub√≥ptima:\n")
    print(problematic_ops)
  } else {
    cat("\n‚úÖ Todas las operaciones tienen performance aceptable\n")
  }
}
```

## Ejercicio Pr√°ctico de Optimizaci√≥n

::: {.callout-note icon="false"}
## üèãÔ∏è Ejercicio 15: Optimizaci√≥n Integral

Dado el siguiente c√≥digo ineficiente, optim√≠zalo usando todas las t√©cnicas aprendidas:

```r
# C√≥digo INEFICIENTE para optimizar
analyze_data_slow <- function(big_data, lookup) {
  results <- data.table()
  
  # Procesar cada regi√≥n por separado
  for(region in unique(big_data$region)) {
    region_data <- big_data[big_data$region == region, ]
    
    # Procesar cada grupo dentro de la regi√≥n
    for(group in unique(region_data$group_major)) {
      group_data <- region_data[region_data$group_major == group, ]
      
      # C√°lculos por grupo
      group_stats <- data.frame(
        region = region,
        group = group,
        count = nrow(group_data),
        mean_value = mean(group_data$value_numeric),
        sum_amount = sum(group_data$amount)
      )
      
      # Join con lookup (ineficiente)
      for(i in 1:nrow(group_stats)) {
        matched_lookup <- lookup[lookup$entity_type == "Premium", ]
        if(nrow(matched_lookup) > 0) {
          group_stats$premium_factor[i] <- mean(matched_lookup$weight_factor)
        }
      }
      
      results <- rbind(results, group_stats)
    }
  }
  
  return(results)
}
```

Optim√≠zalo para:
1. Eliminar todos los bucles
2. Usar operaciones vectorizadas
3. Implementar joins eficientes
4. Minimizar copias de memoria
:::

::: {.callout-tip collapse="true"}
## üí° Soluci√≥n del Ejercicio 15

```{r}
#| label: solucion-ejercicio-15
#| echo: true

# Versi√≥n OPTIMIZADA
analyze_data_fast <- function(big_data, lookup) {
  
  # Pre-calcular el premium_factor una sola vez
  premium_factor <- lookup[entity_type == "Premium", mean(weight_factor)]
  
  # Una sola operaci√≥n vectorizada que reemplaza todos los bucles
  result <- big_data[, .(
    count = .N,
    mean_value = mean(value_numeric),
    sum_amount = sum(amount),
    premium_factor = premium_factor  # Usar valor pre-calculado
  ), by = .(region, group = group_major)]
  
  return(result)
}

# Comparar performance
dt_test_large <- big_dataset[sample(.N, 20000)]  # Dataset m√°s peque√±o para el test
lookup_test <- lookup_data[sample(.N, 5000)]

cat("=== COMPARACI√ìN DE PERFORMANCE ===\n")

# Versi√≥n lenta (simulada de forma m√°s r√°pida para el ejemplo)
tiempo_lento <- system.time({
  # Simulamos la l√≥gica ineficiente pero sin bucles extremos
  result_slow <- dt_test_large[, {
    # M√∫ltiples operaciones separadas (ineficiente)
    temp_results <- list()
    for(i in seq_along(unique(group_major))) {
      group_val <- unique(group_major)[i]
      group_subset <- .SD[group_major == group_val]
      temp_results[[i]] <- data.table(
        region = unique(region),
        group = group_val,
        count = nrow(group_subset),
        mean_value = mean(group_subset$value_numeric),
        sum_amount = sum(group_subset$amount),
        premium_factor = lookup_test[entity_type == "Premium", mean(weight_factor)]
      )
    }
    rbindlist(temp_results)
  }, by = region]
})

# Versi√≥n optimizada
tiempo_rapido <- system.time({
  result_fast <- analyze_data_fast(dt_test_large, lookup_test)
})

cat("M√©todo ineficiente (simulado):", round(tiempo_lento[3], 4), "segundos\n")
cat("M√©todo optimizado:", round(tiempo_rapido[3], 4), "segundos\n")
cat("Mejora de velocidad:", round(tiempo_lento[3] / tiempo_rapido[3], 1), "x m√°s r√°pido\n")

# Verificar resultados equivalentes
cat("Resultados similares:", 
    nrow(result_slow) == nrow(result_fast), 
    all.equal(result_slow$count, result_fast$count), "\n")

print("\nPrimeras filas del resultado optimizado:")
print(head(result_fast))

cat("\n=== T√âCNICAS DE OPTIMIZACI√ìN APLICADAS ===\n")
cat("1. ‚úÖ Eliminaci√≥n completa de bucles for\n")
cat("2. ‚úÖ Una sola operaci√≥n by= vectorizada\n") 
cat("3. ‚úÖ Pre-c√°lculo de valores constantes\n")
cat("4. ‚úÖ Eliminaci√≥n de rbind repetitivo\n")
cat("5. ‚úÖ Sintaxis data.table pura (sin data.frame)\n")
cat("6. ‚úÖ Operaciones vectorizadas nativas\n")
cat("7. ‚úÖ M√≠nimo uso de memoria\n")
```
:::

---

::: {.callout-important}
## üéØ Puntos Clave de Este Cap√≠tulo

1. **Threading autom√°tico** puede acelerar operaciones 2-10x en sistemas multi-core
2. **setkey()** es esencial para datasets >100K filas con consultas repetitivas
3. **√çndices secundarios** con setindex() permiten m√∫ltiples patrones de consulta eficientes
4. **Benchmarking sistem√°tico** revela cuellos de botella reales vs percibidos
5. **Una operaci√≥n data.table vectorizada** puede reemplazar docenas de bucles
6. **Profiling de memoria** es crucial para datasets que se acercan a los l√≠mites de RAM
7. **La optimizaci√≥n correcta** puede resultar en mejoras de 10-100x en casos extremos
:::

El dominio de estas t√©cnicas de optimizaci√≥n te permite trabajar con datasets que de otra manera ser√≠an imposibles de procesar eficientemente. En el pr√≥ximo cap√≠tulo exploraremos las mejores pr√°cticas y patrones que complementan estas optimizaciones.

